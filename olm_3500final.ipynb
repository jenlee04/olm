{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM22wr0HX2YIMUI7+bUFc/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenlee04/olm/blob/main/olm_3500final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U2I2fhJwZAvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s3ckIHrZYgPL",
        "outputId": "5da90534-0409-4b40-d3d0-ad3d4fdf930f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install packages + things needed.\n"
      ],
      "metadata": {
        "id": "abO2yagFaJ-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "import copy"
      ],
      "metadata": {
        "id": "5ok9utPiaJwt"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in datasets\n",
        "off_tes_path = '/content/drive/My Drive/3500final/data/offline_testing/'\n",
        "off_tra_path = '/content/drive/My Drive/3500final/data/offline_training'\n",
        "on_nut_path = '/content/drive/My Drive/3500final/data/online_nuts/'\n",
        "on_spice_path = '/content/drive/My Drive/3500final/data/online_spices/'\n",
        "offline_testing_data = []\n",
        "offline_testing_labels = []\n",
        "offline_training_data = []\n",
        "offline_training_labels = []\n",
        "online_nuts_data = []\n",
        "online_nuts_labels = []\n",
        "online_spices_data = []\n",
        "online_spices_labels = []\n",
        "\n",
        "all_odors_in_order = set()\n",
        "for root, dirs, files in os.walk(off_tes_path):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".csv\"):\n",
        "            df = os.path.join(root, filename)\n",
        "            if os.path.getsize(df) == 0:\n",
        "              print(f\"skipped empty file: {df}\")\n",
        "              continue\n",
        "            try:\n",
        "              df2 = pd.read_csv(df, header = 0)\n",
        "              odor_name = Path(df).parent.name\n",
        "              offline_testing_data.append(df2)\n",
        "              offline_testing_labels.append(odor_name)\n",
        "              all_odors_in_order.add(odor_name)\n",
        "            except Exception as e:\n",
        "              print(f\"error reading {df}: {e}\")\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(off_tra_path):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".csv\"):\n",
        "            df = os.path.join(root, filename)\n",
        "            if os.path.getsize(df) == 0:\n",
        "              print(f\"skipped empty file: {df}\")\n",
        "              continue\n",
        "            try:\n",
        "              df2 = pd.read_csv(df, header = 0)\n",
        "              odor_name = Path(df).parent.name\n",
        "              offline_training_data.append(df2)\n",
        "              offline_training_labels.append(odor_name)\n",
        "              all_odors_in_order.add(odor_name)\n",
        "            except Exception as e:\n",
        "              print(f\"error reading {df}: {e}\")\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(on_nut_path):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".csv\"):\n",
        "            df = os.path.join(root, filename)\n",
        "            if os.path.getsize(df) == 0:\n",
        "              print(f\"skipped empty file: {df}\")\n",
        "              continue\n",
        "            try:\n",
        "              df2 = pd.read_csv(df, header = 0)\n",
        "              odor_name = Path(df).parent.name\n",
        "              online_nuts_data.append(df2)\n",
        "              online_nuts_labels.append(odor_name)\n",
        "              all_odors_in_order.add(odor_name)\n",
        "            except Exception as e:\n",
        "              print(f\"error reading {df}: {e}\")\n",
        "\n",
        "for root, dirs, files in os.walk(on_spice_path):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".csv\"):\n",
        "            df = os.path.join(root, filename)\n",
        "            if os.path.getsize(df) == 0:\n",
        "              print(f\"skipped empty file: {df}\")\n",
        "              continue\n",
        "            try:\n",
        "              df2 = pd.read_csv(df, header = 0)\n",
        "              odor_name = Path(df).parent.name\n",
        "              online_spices_data.append(df2)\n",
        "              online_spices_labels.append(odor_name)\n",
        "              all_odors_in_order.add(odor_name)\n",
        "            except Exception as e:\n",
        "              print(f\"error reading {df}: {e}\")\n",
        "odor_to_label = {name: idx for idx, name in enumerate(sorted(all_odors_in_order))}\n",
        "label_to_odor = {idx: name for name, idx in odor_to_label.items()}\n",
        "\n",
        "# integer vals for labels\n",
        "offline_testing_labels = [odor_to_label[name] for name in offline_testing_labels]\n",
        "offline_training_labels = [odor_to_label[name] for name in offline_training_labels]\n",
        "online_nuts_labels = [odor_to_label[name] for name in online_nuts_labels]\n",
        "online_spices_labels = [odor_to_label[name] for name in online_spices_labels]\n",
        "\n",
        "\n",
        "\n",
        "#print(offline_testing_data)\n",
        "#print(offline_training_data)\n",
        "#print(online_nuts_data)\n",
        "#print(online_spices_data)"
      ],
      "metadata": {
        "id": "UnIUnWYUw-Lm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Olfactory receptors."
      ],
      "metadata": {
        "id": "cYxfvT-uaaGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OlfactoryReceptorLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Purpose: Should mimic olfactory receptors (ORNs) in that certain olfactory receptors\n",
        "  activate for specific odors over others (combinatorial coding)\n",
        "\n",
        "  params\n",
        "  input_dim: number of the input features from sensors\n",
        "  n_receptors: number of artifical olfactory receptors\n",
        "\n",
        "  attributes\n",
        "  self.layer: linear transformation occurs, where it takes in the input dimension\n",
        "  (so the number of \"features\") and the number of receptors - \"output dimension\",\n",
        "  weight matrix is FROZEN (taken from std normal Gaussian). we should not see\n",
        "  any learning occur, we just want to see that the olfactory receptor neurons\n",
        "  fire in a particular way for specific odorants as we observe in biology.\n",
        "\n",
        "  methods\n",
        "  forward: takes gsdata (gas sensor data vector), returns receptor activation with\n",
        "  shape [batch, time, n_receptors] after using ReLU\n",
        "\n",
        "  returns\n",
        "  receptor activations [b,t,n_receptors]\n",
        "  \"\"\"\n",
        "  def __init__(self, input_dim = 12, n_receptors = 100):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.n_receptors = n_receptors\n",
        "\n",
        "    # linear feedforward network\n",
        "    self.layer = nn.Linear(input_dim, n_receptors, bias = False)\n",
        "\n",
        "    # weight initialization and freeze\n",
        "    nn.init.normal_(self.layer.weight, mean = 0.0, std = 1.0)\n",
        "    self.layer.weight.requires_grad = False\n",
        "\n",
        "\n",
        "  def forward(self, gsdata):\n",
        "    # x takes size [batch, time, dimensions]\n",
        "    return torch.relu(self.layer(gsdata))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-fs-qLVwacrV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to project these activations to something that can learn from them."
      ],
      "metadata": {
        "id": "nkDSGt9MDQAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OlfactoryBulb(nn.Module):\n",
        "  \"\"\"\n",
        "  Purpose: learn from the specific activations of the receptors and correlate\n",
        "  them with specific odorants/features of the gas sensor input that is fed into\n",
        "  the receptors. We will be ignoring periglomerular cells (PGs) here\n",
        "  - no fine tuning. * could be added in a later version\n",
        "\n",
        "  params\n",
        "  n_receptors: \"input dimension\" -> specific receptor activations\n",
        "  used as input here.\n",
        "  n_mitral: \"output dimension\" -> number of mitral cells to simulate\n",
        "  lateral_strength: strength of the lateral inhibition between the mitral cells\n",
        "  (val can range between 0 to 1)\n",
        "\n",
        "  attributes\n",
        "  self.alpha: normalization rate parameter\n",
        "  self.eps: epsilon val. that is a small constant, prevents div0\n",
        "  self.gru: Gated Recurrent Unit, simplified ver. of LSTM. used for processing\n",
        "  time series of receptor signals\n",
        "\n",
        "  methods\n",
        "  forward: takes in the receptor activity ([b,t, n_receptors]), returns the mitral\n",
        "  cell activations ([b, t, n_mitral]) after lateral inhibition and normalization\n",
        "  are applied\n",
        "\n",
        "  returns\n",
        "  mitral cell activations [b, t, n_mitral]\n",
        "  \"\"\"\n",
        "  def __init__(self, n_receptors, n_mitral, lateral_strength = 0.3):\n",
        "    super().__init__()\n",
        "    self.alpha = 0.1\n",
        "    self.eps = 1e-6\n",
        "    self.lateral_strength = lateral_strength\n",
        "\n",
        "    # temporal processing - processes the receptor inputs over time\n",
        "    self.gru = nn.GRU(n_receptors, n_mitral, batch_first = True)\n",
        "\n",
        "    # lateral inhib. matrix -> frozen, does not learn, is symmetric and each\n",
        "    # mitral cell inhibits other based on distance\n",
        "    # closer MCs inhibit each other; cannot inhibit further ones\n",
        "    lateral_w = torch.zeros(n_mitral, n_mitral)\n",
        "    for i in range(n_mitral):\n",
        "      for j in range(n_mitral):\n",
        "        if i != j:\n",
        "          dis = abs(i-j) / n_mitral\n",
        "          lateral_w[i,j] = torch.exp(torch.tensor(-dis * 5))\n",
        "    self.register_buffer('lateral_w', lateral_w)\n",
        "\n",
        "\n",
        "  def forward(self, r):\n",
        "    m, _ = self.gru(r) # takes size [B, T, n_mitral]\n",
        "\n",
        "    # lateral inhib. application\n",
        "    B, T, M = m.shape\n",
        "    m_flat = m.reshape(B * T, M)\n",
        "    inhib = F.linear(torch.relu(m_flat), self.lateral_strength * self.lateral_w)\n",
        "    m_inhib = m_flat - inhib\n",
        "    m_inhib = m_inhib.reshape(B, T, M)\n",
        "\n",
        "    # normalization\n",
        "    norm = torch.sqrt((m_inhib**2).mean(dim = -1, keepdim = True) + self.eps)\n",
        "    return m_inhib/(1+self.alpha * norm)"
      ],
      "metadata": {
        "id": "4oV0zjlaaqex"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now interpret via Piriform cortex, which uses Hebbian plasticity."
      ],
      "metadata": {
        "id": "6m-W_T9_GK9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Piriform(nn.Module):\n",
        "  \"\"\"\n",
        "  Purpose: sparse associations, Hebbian learning occurs here. Simulates piriform cortex,\n",
        "  odor patterns should be associated with odor identities. Weight updates.\n",
        "\n",
        "  params\n",
        "  n_mitral: number of mitral cell inputs from OB\n",
        "  n_piric: number of piriform cortex neurons\n",
        "  thresh = activation threshold, used for sparse coding\n",
        "\n",
        "  attributes\n",
        "  self.w: (nonfrozen) weights with shape [n_piric, n_mitral]\n",
        "  self.thresh: sparsity threshold (if > self.thresh, activates)\n",
        "\n",
        "  methods\n",
        "  forward: takes mitral cell activations and returns piriform cortex activations\n",
        "  with shape [batch, n_piric], first takes temporal mean, then linear transformation\n",
        "  and thresh. difference, then ReLU\n",
        "\n",
        "  hebbian_update: updates weights via Hebbian learning, takes the mitral cell activations,\n",
        "  piriform activations, and a hebbian learning rate (usually small like 1e-4)\n",
        "  \"\"\"\n",
        "  def __init__(self, n_mitral, n_piric, thresh = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # estalbish the weights\n",
        "    self.w = nn.Parameter(torch.randn(n_piric, n_mitral)* 0.1)\n",
        "    self.thresh = thresh\n",
        "\n",
        "\n",
        "  # same idea as in receptor layer\n",
        "  def forward(self, m):\n",
        "    # takes in mitral cell activity and the weights\n",
        "    mean_m= m.mean(dim = 1)\n",
        "    activ = torch.relu(F.linear(mean_m, self.w) - self.thresh)\n",
        "    return activ\n",
        "\n",
        "  # no need for gradient tracking here, no backprop\n",
        "  @torch.no_grad()\n",
        "  def hebbian_update(self, m, activ, lr = 1e-4):\n",
        "    # hebbian rule is that the change in weights is proportional to the feed forward\n",
        "    # activity multiplied by the mitral cell activity, matrix multiplicatoin\n",
        "    # y.T : (n_pc, B) and m: (B, n_mitral), gives n_pc by n_mitral weight matrix\n",
        "    mean_m = m.mean(dim =1)\n",
        "    delw = (activ.unsqueeze(-1)*mean_m.unsqueeze(1)).mean(dim=0) # hebbian weight changes\n",
        "\n",
        "    # learning rate applied and update weights\n",
        "    self.w.data += lr * delw\n"
      ],
      "metadata": {
        "id": "zxSQ2AcOGKoQ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full model"
      ],
      "metadata": {
        "id": "Kx3q-fLFSLUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OLM(nn.Module):\n",
        "  \"\"\"\n",
        "  Purpose: everything put together! Should go from the receptor to bulb to piriform to\n",
        "  an output head that classifies the odor.\n",
        "\n",
        "  params\n",
        "  input_dim: number of dimensions for input (12 in this case)\n",
        "  n_receptors: number of olfactory receptors\n",
        "  n_mitral: number of mitral cells\n",
        "  n_piric: number of piriform cortex cells\n",
        "  n_class: number of odor classes for classification (50 in this case)\n",
        "  lateral_strength: lateral inhibition val for olfactory bulb\n",
        "\n",
        "  attributes\n",
        "  self.receptors: the olfactory receptors\n",
        "  self.bulb: olfactory bulb\n",
        "  self.piriform: piriform cortex\n",
        "  self.classifier: one layer (unfrozen weights) network with supervision, classifies\n",
        "  odors\n",
        "\n",
        "  methods\n",
        "  forward: takes sensor input and at the very end outputs the corresponding odor\n",
        "  label\n",
        "\n",
        "  returns\n",
        "  logits - predictions for the odor identity [batch, n_class]\n",
        "  bulb - mitral cell activations [batch, time, n_piric]\n",
        "  pc - piriform cortex activations [batch, n_piric]\n",
        "  \"\"\"\n",
        "  def __init__(self, input_dim, n_receptors, n_mitral, n_piric, n_class, lateral_strength = 0.3):\n",
        "    super().__init__()\n",
        "    self.receptors = OlfactoryReceptorLayer(input_dim, n_receptors)\n",
        "    self.bulb = OlfactoryBulb(n_receptors, n_mitral, lateral_strength)\n",
        "    self.piriform = Piriform(n_mitral, n_piric)\n",
        "    self.classifier = nn.Linear(n_piric, n_class)\n",
        "\n",
        "  def forward(self, inp):\n",
        "    rec = self.receptors(inp)\n",
        "    bulb = self.bulb(rec)\n",
        "    pc = self.piriform(bulb)\n",
        "    logits = self.classifier(pc)\n",
        "\n",
        "    return logits, bulb, pc\n",
        "\n"
      ],
      "metadata": {
        "id": "Njm4MkJ9GJ55"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now make a class so we can load in the datasets from earlier."
      ],
      "metadata": {
        "id": "hJhdRzuhTQQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmellNetData(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  Purpose: Used for dataset loading and preprocessing via FOTD when needed.\n",
        "\n",
        "  params\n",
        "  dataframes: the list of the gas sensor data csv files\n",
        "  labels: integer labels that correspond to each dataframe\n",
        "  mean: mean vals for normalization (one per sensor data)\n",
        "  std: std val for normalization\n",
        "  use_fotd: takes True/False, toggle to apply FOTD\n",
        "  p: lag parameter for FOTD (p = 25 here)\n",
        "  max_len: maximum sequence length, needed to pad/truncate because not every csv\n",
        "  file has the same amount of timesteps/rows\n",
        "\n",
        "  attributes\n",
        "  self.sensor_cols: names of the 12 features of the input data\n",
        "  self.max_len: fixed seq length\n",
        "\n",
        "  methods\n",
        "  fotd: applies FOTD transformation, input is the time series (time, features)\n",
        "  and output is the derivative time series\n",
        "  __len__(): returns number of samples in the dataset\n",
        "  __getitem__(idx): retrieve and preprocess a single sample at index idx\n",
        "  \"\"\"\n",
        "  def __init__(self, dataframes, labels, mean, std, use_fotd = False, p = 25, max_len = None):\n",
        "    self.dataframes = dataframes\n",
        "    self.labels = labels\n",
        "    self.use_fotd = use_fotd\n",
        "    self.p = p\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "\n",
        "    self.max_len = max_len or max(len(df) for df in dataframes)\n",
        "\n",
        "    self.sensor_cols = [\"NO2\", \"C2H5OH\", \"VOC\", \"CO\", \"Alcohol\",\n",
        "                        \"LPG\", \"Benzene\", \"Temperature\", \"Pressure\",\n",
        "                        \"Humidity\", \"Gas_Resistance\", \"Altitude\"]\n",
        "  def fotd(self, x):\n",
        "    dx = x[self.p:] - x[:-self.p]\n",
        "    pad = torch.zeros(self.p, x.shape[1])\n",
        "    return torch.cat([pad, dx], dim = 0)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframes)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    df = self.dataframes[idx]\n",
        "    x = torch.tensor(df[self.sensor_cols].values, dtype = torch.float32)\n",
        "\n",
        "    # dataset level normalization because of varying row number\n",
        "    x = (x-self.mean) / (self.std + 1e-6)\n",
        "    if self.use_fotd:\n",
        "      x = self.fotd(x)\n",
        "\n",
        "    # pad/truncate the time dimension\n",
        "    T, D = x.shape\n",
        "    if T < self.max_len:\n",
        "      pad = torch.zeros(self.max_len - T, D)\n",
        "      x = torch.cat([x,pad], dim = 0)\n",
        "    else:\n",
        "      x = x[:self.max_len]\n",
        "\n",
        "    y = torch.tensor(self.labels[idx], dtype = torch.long)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "mrx3dOKTScFD"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to use in training"
      ],
      "metadata": {
        "id": "Z94q3gzSTZgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(model, loader, optimizer, criterion, device, use_hebbian = False, hebb_lr = 1e-3):\n",
        "  \"\"\"\n",
        "  Purpose: execute an epoch of training w/ optional updates to Hebbian plasticity\n",
        "\n",
        "  params\n",
        "  model: OLM to train\n",
        "  loader: training data loader\n",
        "  optimizer: pytorch optimizer, in this case Adam\n",
        "  criterion: loss function, in this case cross entropy\n",
        "  device: cuda or cpu, used A100\n",
        "  use_hebbian: if True, hebbian updates are applied to piriform (happens after\n",
        "  gradient based updates)\n",
        "  hebb_lr: learning rate for hebbian updates\n",
        "\n",
        "  returns\n",
        "  avg_loss: average loss (across all batches)\n",
        "  acc: training accuracy (computed from argmax of logits - predictions)\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for x, y in loader:\n",
        "    x,y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits, bulb, pc = model(x)\n",
        "    loss = criterion(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if use_hebbian:\n",
        "      with torch.no_grad():\n",
        "        model.piriform.hebbian_update(bulb, pc, lr = hebb_lr)\n",
        "\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    _, predicted = logits.max(1)\n",
        "    total += y.size(0)\n",
        "    correct += predicted.eq(y).sum().item()\n",
        "\n",
        "  avg_loss = total_loss / len(loader)\n",
        "  acc = 100. * correct / total\n",
        "\n",
        "  return avg_loss, acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, k = 5):\n",
        "  \"\"\"\n",
        "  Purpose: evaluate model perform. on test data with topk accuracy.\n",
        "  using top1 and top5 for this.\n",
        "\n",
        "  params\n",
        "  model: olm in this case\n",
        "  loader: dataloader for test\n",
        "  device: cuda or cpu, used A100\n",
        "  k: number of top predictions to consider\n",
        "\n",
        "  returns\n",
        "  top1_acc: top1 accuracy (is the model's first choice correct?)\n",
        "  topk_acc: topk accuracy (is the correct choice within the model's top k predictions)\n",
        "  k here = 5\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  correct1 = 0\n",
        "  correctk = 0\n",
        "  total = 0\n",
        "\n",
        "  for x, y in loader:\n",
        "    x,y = x.to(device), y.to(device)\n",
        "    logits, _, _ = model(x)\n",
        "\n",
        "    _, topk = logits.topk(k, dim = 1)\n",
        "    correct1 += (topk[:, 0] == y).sum().item()\n",
        "    correctk += (topk == y.unsqueeze(1)).any(dim=1).sum().item()\n",
        "    total += y.size(0)\n",
        "  top1_acc = 100. * correct1 / total\n",
        "  topk_acc = 100. * correctk / total\n",
        "\n",
        "  return top1_acc, topk_acc\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_by_category(model, dataframes, labels, device, ingredient_to_category, label_to_odor,\n",
        "                         mean, std, use_fotd = False, p = 25, max_len = None,\n",
        "                         k = 5):\n",
        "  \"\"\"\n",
        "  Purpose: analyzes model performance based on category like they do in Feng et\n",
        "  al., 2025 - categories as given by the paper\n",
        "\n",
        "  params\n",
        "  model: olm in this case\n",
        "  dataframes: raw gas sensor data\n",
        "  labels: integer labels for each SAMPLE\n",
        "  device: cuda or cpu, used A100\n",
        "  ingredient_to_category: dictionary, maps the odor label to the category\n",
        "  label_to_odor: dictionary, maps the integer label to the odor label\n",
        "  k: topk parameter, only really used k = 5 here\n",
        "\n",
        "  returns\n",
        "  category_results: dictionary, maps the category name to the performance metrics\n",
        "  (top1_acc, top5_acc)\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  sensor_cols = [\"NO2\", \"C2H5OH\", \"VOC\", \"CO\", \"Alcohol\",\n",
        "                        \"LPG\", \"Benzene\", \"Temperature\", \"Pressure\",\n",
        "                        \"Humidity\", \"Gas_Resistance\", \"Altitude\"]\n",
        "  if max_len is None:\n",
        "    max_len = max(len(df) for df in dataframes)\n",
        "\n",
        "  def fotd(x, p):\n",
        "    dx = x[p:] - x[:-p]\n",
        "    pad = torch.zeros(p, x.shape[1])\n",
        "    return torch.cat([pad, dx], dim = 0)\n",
        "\n",
        "  category_stats = {}\n",
        "  for category in set(ingredient_to_category.values()):\n",
        "    category_stats[category] = {\n",
        "        'correct1': 0,\n",
        "        'correct5' :0,\n",
        "        'total': 0\n",
        "    }\n",
        "  for i, (df, label) in enumerate(zip(dataframes, labels)):\n",
        "    odor_name = label_to_odor[label]\n",
        "    category = ingredient_to_category.get(odor_name, 'Unknown')\n",
        "\n",
        "    x = torch.tensor(df[sensor_cols].values, dtype = torch.float32)\n",
        "    x = (x - train_mean) / (train_std + 1e-6)\n",
        "\n",
        "    if use_fotd:\n",
        "      x = fotd(x, p)\n",
        "\n",
        "    T, D = x.shape\n",
        "    max_len = train_dataset.max_len\n",
        "    if T < max_len:\n",
        "      pad = torch.zeros(max_len - T, D)\n",
        "      x = torch.cat([x, pad], dim = 0)\n",
        "    else:\n",
        "      x = x[:max_len]\n",
        "\n",
        "    x = x.unsqueeze(0).to(device)\n",
        "\n",
        "    # predictions\n",
        "    logits, _, _ = model(x)\n",
        "    _, topk = logits.topk(k, dim = 1)\n",
        "\n",
        "    # updates\n",
        "    y_tensor = torch.tensor([label]).to(device)\n",
        "    category_stats[category]['correct1'] += (topk[:,0] == y_tensor).sum().item()\n",
        "    category_stats[category]['correct5'] += (topk == y_tensor.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "    category_stats[category]['total'] += 1\n",
        "\n",
        "  category_results = {}\n",
        "  for category, stats in category_stats.items():\n",
        "    if stats['total'] > 0:\n",
        "      category_results[category] = {\n",
        "          'top1_acc': 100. * stats['correct1'] / stats['total'],\n",
        "          'top5_acc': 100. * stats['correct5'] / stats['total'],\n",
        "          'n_samples' : stats['total']\n",
        "      }\n",
        "\n",
        "  return category_results"
      ],
      "metadata": {
        "id": "4YZpYqHITbJg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading in the datasets and data setup, model initialization"
      ],
      "metadata": {
        "id": "b7lkcGXLtnxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sens_stat(dataframes, sensor_cols):\n",
        "  \"\"\"\n",
        "  Purpose: compute the mean and std of sensor readings across the entire dataset\n",
        "  so that normalization can be carried out.\n",
        "\n",
        "  params\n",
        "  dataframes: all the csv files\n",
        "  sensor_cols: names of the features in the gas sensor data\n",
        "\n",
        "  returns\n",
        "  mean: mean of each feature\n",
        "  std: standard deviation of each feature\n",
        "  \"\"\"\n",
        "  all_inp = []\n",
        "  for df in dataframes:\n",
        "    all_inp.append(torch.tensor(df[sensor_cols].values, dtype = torch.float32))\n",
        "  all_inp = torch.cat(all_inp, dim = 0)\n",
        "\n",
        "  mean = all_inp.mean(dim=0)\n",
        "  std = all_inp.std(dim =0)\n",
        "  return mean, std\n",
        "\n",
        "sensor_cols = [\"NO2\", \"C2H5OH\", \"VOC\", \"CO\", \"Alcohol\",\n",
        "                        \"LPG\", \"Benzene\", \"Temperature\", \"Pressure\",\n",
        "                        \"Humidity\", \"Gas_Resistance\", \"Altitude\"]\n",
        "# taken directly from smellnet's code\n",
        "ingredient_to_category = {\n",
        "    # Nuts\n",
        "    \"peanuts\": \"Nuts\",\n",
        "    \"cashew\": \"Nuts\",\n",
        "    \"chestnuts\": \"Nuts\",\n",
        "    \"pistachios\": \"Nuts\",\n",
        "    \"almond\": \"Nuts\",\n",
        "    \"hazelnut\": \"Nuts\",\n",
        "    \"walnuts\": \"Nuts\",\n",
        "    \"pecans\": \"Nuts\",\n",
        "    \"brazil_nut\": \"Nuts\",\n",
        "    \"pili_nut\": \"Nuts\",\n",
        "\n",
        "    # Spices\n",
        "    \"cumin\": \"Spices\",\n",
        "    \"star_anise\": \"Spices\",\n",
        "    \"nutmeg\": \"Spices\",\n",
        "    \"cloves\": \"Spices\",\n",
        "    \"ginger\": \"Spices\",\n",
        "    \"allspice\": \"Spices\",\n",
        "    \"chervil\": \"Spices\",\n",
        "    \"mustard\": \"Spices\",\n",
        "    \"cinnamon\": \"Spices\",\n",
        "    \"saffron\": \"Spices\",\n",
        "\n",
        "    # Herbs\n",
        "    \"angelica\": \"Herbs\",\n",
        "    \"garlic\": \"Herbs\",\n",
        "    \"chives\": \"Herbs\",\n",
        "    \"turnip\": \"Herbs\",\n",
        "    \"dill\": \"Herbs\",\n",
        "    \"mugwort\": \"Herbs\",\n",
        "    \"chamomile\": \"Herbs\",\n",
        "    \"coriander\": \"Herbs\",\n",
        "    \"oregano\": \"Herbs\",\n",
        "    \"mint\": \"Herbs\",\n",
        "\n",
        "    # Fruits\n",
        "    \"kiwi\": \"Fruits\",\n",
        "    \"pineapple\": \"Fruits\",\n",
        "    \"banana\": \"Fruits\",\n",
        "    \"lemon\": \"Fruits\",\n",
        "    \"mandarin_orange\": \"Fruits\",\n",
        "    \"strawberry\": \"Fruits\",\n",
        "    \"apple\": \"Fruits\",\n",
        "    \"mango\": \"Fruits\",\n",
        "    \"peach\": \"Fruits\",\n",
        "    \"pear\": \"Fruits\",\n",
        "\n",
        "    # Vegetables\n",
        "    \"cauliflower\": \"Vegetables\",\n",
        "    \"brussel_sprouts\": \"Vegetables\",\n",
        "    \"broccoli\": \"Vegetables\",\n",
        "    \"sweet_potato\": \"Vegetables\",\n",
        "    \"asparagus\": \"Vegetables\",\n",
        "    \"avocado\": \"Vegetables\",\n",
        "    \"radish\": \"Vegetables\",\n",
        "    \"tomato\": \"Vegetables\",\n",
        "    \"potato\": \"Vegetables\",\n",
        "    \"cabbage\": \"Vegetables\",\n",
        "}\n",
        "train_mean, train_std = compute_sens_stat(offline_training_data, sensor_cols)\n",
        "\n",
        "# datasets\n",
        "# training data\n",
        "train_dataset = SmellNetData(offline_training_data, offline_training_labels,\n",
        "                             mean = train_mean, std = train_std, use_fotd = False)\n",
        "\n",
        "# testing data\n",
        "test_dataset = SmellNetData(offline_testing_data, offline_testing_labels,\n",
        "                            mean = train_mean, std = train_std, use_fotd = False)\n",
        "\n",
        "# fotd on\n",
        "train_datasetf = SmellNetData(offline_training_data, offline_training_labels,\n",
        "                             mean = train_mean, std = train_std, use_fotd = True, p = 25)\n",
        "test_datasetf = SmellNetData(offline_testing_data, offline_testing_labels,\n",
        "                            mean = train_mean, std = train_std, use_fotd = True, p = 25)\n",
        "# online data\n",
        "\n",
        "#absolute\n",
        "online_nut_dataseta = SmellNetData(online_nuts_data, online_nuts_labels, mean = train_mean,\n",
        "                                  std = train_std, use_fotd = False)\n",
        "online_spices_dataseta = SmellNetData(online_spices_data, online_spices_labels,\n",
        "                                     mean = train_mean, std = train_std, use_fotd =False)\n",
        "\n",
        "#fotd on\n",
        "online_nut_datasetf = SmellNetData(online_nuts_data, online_nuts_labels, mean = train_mean,\n",
        "                                  std = train_std, use_fotd = True, p = 25)\n",
        "online_spices_datasetf = SmellNetData(online_spices_data, online_spices_labels,\n",
        "                                     mean = train_mean, std = train_std, use_fotd = True, p = 25)\n",
        "\n",
        "# dataloaders\n",
        "batch = 16\n",
        "#offline\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch, shuffle = False)\n",
        "\n",
        "train_loaderf = DataLoader(train_datasetf, batch_size = batch, shuffle = True)\n",
        "test_loaderf = DataLoader(test_datasetf, batch_size = batch, shuffle = False)\n",
        "\n",
        "#online\n",
        "online_nut_loadera = DataLoader(online_nut_dataseta, batch_size = batch, shuffle = False)\n",
        "online_spices_loadera = DataLoader(online_spices_dataseta, batch_size = batch, shuffle = False)\n",
        "online_nut_loaderf = DataLoader(online_nut_datasetf, batch_size = batch, shuffle = False)\n",
        "online_spices_loaderf = DataLoader(online_spices_datasetf, batch_size = batch, shuffle = False)\n",
        "\n",
        "# sanity check\n",
        "n_classes = len(odor_to_label)\n",
        "print(f\"\\nNumber of Classes: {n_classes}\")\n",
        "\n",
        "\n",
        "\n",
        "# model initialization\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WapQWPTWto3o",
        "outputId": "383f77bb-93d3-4a26-90a3-27a7552e65a1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of Classes: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "PIi3pYDZL0UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-4\n",
        "n_receptors = 500\n",
        "n_mitral = 200\n",
        "n_piric = 400\n",
        "n_class = 50\n",
        "\n",
        "\n",
        "#model\n",
        "olm1 = OLM(input_dim = 12, n_receptors = n_receptors, n_mitral = n_mitral, n_piric = n_piric, n_class = n_classes,\n",
        "           lateral_strength = 0.3).to(device)\n",
        "optimizer1 = torch.optim.Adam(olm1.parameters(), lr = lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 200\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_top1accs = []\n",
        "test_top5accs = []\n",
        "\n",
        "results1 = {}\n",
        "\n",
        "# TEST 1: absolute readings\n",
        "for epoch in range(num_epochs):\n",
        "  # training\n",
        "  train_loss, train_acc = training(\n",
        "      olm1, train_loader, optimizer1, criterion, device, use_hebbian = True, hebb_lr = 1e-4\n",
        "  ) # hebbian can be toggled to true\n",
        "  test_top1, test_top5 = evaluate(olm1, test_loader, device, k = 5)\n",
        "\n",
        "  # storage\n",
        "  train_losses.append(train_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  test_top1accs.append(test_top1)\n",
        "  test_top5accs.append(test_top5)\n",
        "\n",
        "  if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\" Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\" Test top1: {test_top1:.2f}%, Test top5: {test_top5:.2f}%\")\n",
        "\n",
        "\n",
        "results1['train_losses'] = train_losses\n",
        "results1['train_accs'] = train_accs\n",
        "results1['test_top1accs'] = test_top1accs\n",
        "results1['test_top5accs'] = test_top5accs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fb7PvMvfT0az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d167b5fc-8db4-4f1d-cd99-a87fef67595d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            " Train Loss: 8.5215, Train Acc: 2.00%\n",
            " Test top1: 2.00%, Test top5: 10.00%\n",
            "Epoch 10/200\n",
            " Train Loss: 4.2841, Train Acc: 4.80%\n",
            " Test top1: 6.00%, Test top5: 24.00%\n",
            "Epoch 20/200\n",
            " Train Loss: 3.6645, Train Acc: 15.20%\n",
            " Test top1: 24.00%, Test top5: 40.00%\n",
            "Epoch 30/200\n",
            " Train Loss: 2.9782, Train Acc: 24.80%\n",
            " Test top1: 22.00%, Test top5: 68.00%\n",
            "Epoch 40/200\n",
            " Train Loss: 2.5874, Train Acc: 35.60%\n",
            " Test top1: 42.00%, Test top5: 76.00%\n",
            "Epoch 50/200\n",
            " Train Loss: 2.1916, Train Acc: 44.00%\n",
            " Test top1: 32.00%, Test top5: 80.00%\n",
            "Epoch 60/200\n",
            " Train Loss: 1.8644, Train Acc: 41.20%\n",
            " Test top1: 44.00%, Test top5: 86.00%\n",
            "Epoch 70/200\n",
            " Train Loss: 1.6832, Train Acc: 46.80%\n",
            " Test top1: 56.00%, Test top5: 80.00%\n",
            "Epoch 80/200\n",
            " Train Loss: 1.5480, Train Acc: 52.80%\n",
            " Test top1: 52.00%, Test top5: 84.00%\n",
            "Epoch 90/200\n",
            " Train Loss: 1.3254, Train Acc: 59.20%\n",
            " Test top1: 62.00%, Test top5: 86.00%\n",
            "Epoch 100/200\n",
            " Train Loss: 1.1595, Train Acc: 61.60%\n",
            " Test top1: 60.00%, Test top5: 92.00%\n",
            "Epoch 110/200\n",
            " Train Loss: 1.2771, Train Acc: 54.80%\n",
            " Test top1: 60.00%, Test top5: 92.00%\n",
            "Epoch 120/200\n",
            " Train Loss: 1.0347, Train Acc: 64.00%\n",
            " Test top1: 62.00%, Test top5: 96.00%\n",
            "Epoch 130/200\n",
            " Train Loss: 1.0000, Train Acc: 68.80%\n",
            " Test top1: 60.00%, Test top5: 96.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST 2: now with fotd pre-processing\n",
        "olm2 = OLM(input_dim = 12, n_receptors = n_receptors, n_mitral = n_mitral, n_piric = n_piric, n_class = n_classes,\n",
        "           lateral_strength = 0.3).to(device)\n",
        "optimizer2 = torch.optim.Adam(olm2.parameters(), lr = lr)\n",
        "\n",
        "training_loss = []\n",
        "training_acc = []\n",
        "testing_top1 = []\n",
        "testing_top5 = []\n",
        "\n",
        "results2 = {}\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss, train_acc = training(\n",
        "      olm2, train_loaderf, optimizer2, criterion, device, use_hebbian = True, hebb_lr = 1e-4\n",
        "  )\n",
        "  test_top1, test_top5 = evaluate(olm2, test_loaderf, device, k = 5)\n",
        "\n",
        "  # storage\n",
        "  training_loss.append(train_loss)\n",
        "  training_acc.append(train_acc)\n",
        "  testing_top1.append(test_top1)\n",
        "  testing_top5.append(test_top5)\n",
        "\n",
        "  if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\" Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\" Test top1: {test_top1:.2f}%, Test top5: {test_top5:.2f}%\")\n",
        "\n",
        "\n",
        "results2['train_losses'] = training_loss\n",
        "results2['train_accs'] = training_acc\n",
        "results2['test_top1accs'] = testing_top1\n",
        "results2['test_top5accs'] = testing_top5\n"
      ],
      "metadata": {
        "id": "Q4AeHiXclYIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 3: evaluation of online learning using nuts and spices datasets\n",
        "# absolute, nuts\n",
        "olm3 = copy.deepcopy(olm1)\n",
        "olm3.bulb.gru.flatten_parameters()\n",
        "og_classifier_weight = olm3.classifier.weight.data.clone()\n",
        "og_classifier_bias = olm3.classifier.bias.data.clone() if olm3.classifier.bias is not None else None\n",
        "og_piriform_w = olm3.piriform.w.data.clone()\n",
        "\n",
        "#before and after\n",
        "correct1_before_nuts = 0\n",
        "correct5_before_nuts = 0\n",
        "\n",
        "correct1_after_nuts = 0\n",
        "correct5_after_nuts = 0\n",
        "total_nuts = 0\n",
        "\n",
        "# optimizer\n",
        "adapt_lr = 1e-5\n",
        "n_adapt_steps =5\n",
        "adapt_optimizer = torch.optim.Adam(olm3.classifier.parameters(), lr = adapt_lr)\n",
        "\n",
        "for x, y in online_nut_loadera:\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  # before adap.\n",
        "  olm3.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, bulb, pc = olm3(x)\n",
        "    _, topk = logits.topk(5, dim = 1)\n",
        "    correct1_before_nuts += (topk[:,0] == y).sum().item()\n",
        "    correct5_before_nuts += (topk == y.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "\n",
        "  # online adaptation\n",
        "  olm3.train()\n",
        "  for step in range(n_adapt_steps):\n",
        "    adapt_optimizer.zero_grad()\n",
        "    logits, bulb, pc = olm3(x)\n",
        "    loss = criterion(logits, y)\n",
        "    loss.backward()\n",
        "    adapt_optimizer.step()\n",
        "\n",
        "    # hebb update during adap.\n",
        "    with torch.no_grad():\n",
        "      olm3.piriform.hebbian_update(bulb, pc, lr = 1e-5)\n",
        "\n",
        "  # after adap.\n",
        "  olm3.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, _ , _ = olm3(x)\n",
        "    _, topk = logits.topk(5, dim = 1)\n",
        "    correct1_after_nuts += (topk[:,0] == y).sum().item()\n",
        "    correct5_after_nuts += (topk == y.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "\n",
        "  total_nuts += y.size(0)\n",
        "#accuracy calcs\n",
        "top1_before_nuts = 100. * correct1_before_nuts / total_nuts\n",
        "top5_before_nuts = 100. * correct5_before_nuts / total_nuts\n",
        "top1_after_nuts = 100. * correct1_after_nuts / total_nuts\n",
        "top5_after_nuts = 100. * correct5_after_nuts / total_nuts\n",
        "\n",
        "improvement1na = top1_after_nuts - top1_before_nuts\n",
        "improvement5na = top5_after_nuts - top5_before_nuts\n",
        "\n",
        "# reset for use for spices\n",
        "olm3.classifier.weight.data = og_classifier_weight.clone()\n",
        "if og_classifier_bias is not None:\n",
        "    olm3.classifier.bias.data = og_classifier_bias.clone()\n",
        "olm3.piriform.w.data = og_piriform_w.clone()\n",
        "\n",
        "\n",
        "# fotd, nuts\n",
        "olm4 = copy.deepcopy(olm2)\n",
        "olm4.bulb.gru.flatten_parameters()\n",
        "og_classifier_weightf = olm4.classifier.weight.data.clone()\n",
        "og_classifier_biasf = olm4.classifier.bias.data.clone() if olm4.classifier.bias is not None else None\n",
        "og_piriform_wf = olm4.piriform.w.data.clone()\n",
        "\n",
        "correct1_before_nutsf = 0\n",
        "correct5_before_nutsf = 0\n",
        "\n",
        "correct1_after_nutsf = 0\n",
        "correct5_after_nutsf = 0\n",
        "total_nutsf = 0\n",
        "\n",
        "# optimizer\n",
        "adapt_lrf = 1e-5\n",
        "n_adapt_stepsf =5\n",
        "adapt_optimizerf = torch.optim.Adam(olm4.classifier.parameters(), lr = adapt_lrf)\n",
        "\n",
        "for x, y in online_nut_loaderf:\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  # before adap.\n",
        "  olm4.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, bulb, pc = olm4(x)\n",
        "    _, topk = logits.topk(5, dim = 1)\n",
        "    correct1_before_nutsf += (topk[:,0] == y).sum().item()\n",
        "    correct5_before_nutsf += (topk == y.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "\n",
        "  # online adaptation\n",
        "  olm4.train()\n",
        "  for step in range(n_adapt_stepsf):\n",
        "    adapt_optimizerf.zero_grad()\n",
        "    logits, bulb, pc = olm4(x)\n",
        "    loss = criterion(logits, y)\n",
        "    loss.backward()\n",
        "    adapt_optimizerf.step()\n",
        "\n",
        "    # hebb update during adap.\n",
        "    with torch.no_grad():\n",
        "      olm4.piriform.hebbian_update(bulb, pc, lr = 1e-5)\n",
        "  # after adap.\n",
        "  olm4.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, _ , _ = olm4(x)\n",
        "    _, topk = logits.topk(5, dim = 1)\n",
        "    correct1_after_nutsf += (topk[:,0] == y).sum().item()\n",
        "    correct5_after_nutsf += (topk == y.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "\n",
        "  total_nutsf += y.size(0)\n",
        "#accuracy calcs\n",
        "top1_before_nutsf = 100. * correct1_before_nutsf / total_nutsf\n",
        "top5_before_nutsf = 100. * correct5_before_nutsf / total_nutsf\n",
        "top1_after_nutsf = 100. * correct1_after_nutsf / total_nutsf\n",
        "top5_after_nutsf = 100. * correct5_after_nutsf / total_nutsf\n",
        "\n",
        "improvement1nf = top1_after_nutsf - top1_before_nutsf\n",
        "improvement5nf = top5_after_nutsf - top5_before_nutsf\n",
        "\n",
        "# reset for use for spices\n",
        "olm4.classifier.weight.data = og_classifier_weightf.clone()\n",
        "if og_classifier_biasf is not None:\n",
        "    olm4.classifier.bias.data = og_classifier_biasf.clone()\n",
        "olm4.piriform.w.data = og_piriform_wf.clone()\n",
        "\n",
        "\n",
        "#absolute, spices\n",
        "#before and after\n",
        "correct1_before_spices = 0\n",
        "correct5_before_spices = 0\n",
        "\n",
        "correct1_after_spices = 0\n",
        "correct5_after_spices = 0\n",
        "total_spices = 0\n",
        "\n",
        "# optimizer\n",
        "adapt_optimizers = torch.optim.Adam(olm3.classifier.parameters(), lr = adapt_lr)\n",
        "\n",
        "for x, y in online_spices_loadera:\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  # before adap.\n",
        "  olm3.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, bulb, pc = olm3(x)\n",
        "    _, topk = logits.topk(5, dim = 1)\n",
        "    correct1_before_spices += (topk[:,0] == y).sum().item()\n",
        "    correct5_before_spices += (topk == y.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "\n",
        "  # online adaptation\n",
        "  olm3.train()\n",
        "  for step in range(n_adapt_steps):\n",
        "    adapt_optimizers.zero_grad()\n",
        "    logits, bulb, pc = olm3(x)\n",
        "    loss = criterion(logits, y)\n",
        "    loss.backward()\n",
        "    adapt_optimizers.step()\n",
        "\n",
        "    # hebb update during adap.\n",
        "    with torch.no_grad():\n",
        "      olm3.piriform.hebbian_update(bulb, pc, lr = 1e-5)\n",
        "\n",
        "  # after adap.\n",
        "  olm3.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, _ , _ = olm3(x)\n",
        "    _, topk = logits.topk(5, dim = 1)\n",
        "    correct1_after_spices += (topk[:,0] == y).sum().item()\n",
        "    correct5_after_spices += (topk == y.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "\n",
        "  total_spices += y.size(0)\n",
        "\n",
        "#accuracy calcs\n",
        "top1_before_spices = 100. * correct1_before_spices / total_spices\n",
        "top5_before_spices = 100. * correct5_before_spices / total_spices\n",
        "top1_after_spices = 100. * correct1_after_spices / total_spices\n",
        "top5_after_spices = 100. * correct5_after_spices / total_spices\n",
        "\n",
        "improvement1sa = top1_after_spices - top1_before_spices\n",
        "improvement5sa = top5_after_spices - top5_before_spices\n",
        "\n",
        "# reset just in general\n",
        "olm3.classifier.weight.data = og_classifier_weight.clone()\n",
        "if og_classifier_bias is not None:\n",
        "    olm3.classifier.bias.data = og_classifier_bias.clone()\n",
        "olm3.piriform.w.data = og_piriform_w.clone()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# fotd, spices\n",
        "correct1_before_spicesf = 0\n",
        "correct5_before_spicesf = 0\n",
        "\n",
        "correct1_after_spicesf = 0\n",
        "correct5_after_spicesf = 0\n",
        "total_spicesf = 0\n",
        "\n",
        "# optimizer\n",
        "adapt_lrs = 1e-5\n",
        "n_adapt_stepss =5\n",
        "adapt_optimizers = torch.optim.Adam(olm4.classifier.parameters(), lr = adapt_lrf)\n",
        "\n",
        "for x, y in online_spices_loaderf:\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  # before adap.\n",
        "  olm4.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, bulb, pc = olm4(x)\n",
        "    _, topk = logits.topk(5, dim = 1)\n",
        "    correct1_before_spicesf += (topk[:,0] == y).sum().item()\n",
        "    correct5_before_spicesf += (topk == y.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "\n",
        "  # online adaptation\n",
        "  olm4.train()\n",
        "  for step in range(n_adapt_stepss):\n",
        "    adapt_optimizers.zero_grad()\n",
        "    logits, bulb, pc = olm4(x)\n",
        "    loss = criterion(logits, y)\n",
        "    loss.backward()\n",
        "    adapt_optimizers.step()\n",
        "\n",
        "    # hebb update during adap.\n",
        "    with torch.no_grad():\n",
        "      olm4.piriform.hebbian_update(bulb, pc, lr = 1e-5)\n",
        "  # after adap.\n",
        "  olm4.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, _ , _ = olm4(x)\n",
        "    _, topk = logits.topk(5, dim = 1)\n",
        "    correct1_after_spicesf += (topk[:,0] == y).sum().item()\n",
        "    correct5_after_spicesf += (topk == y.unsqueeze(1)).any(dim = 1).sum().item()\n",
        "\n",
        "  total_spicesf += y.size(0)\n",
        "#accuracy calcs\n",
        "top1_before_spicesf = 100. * correct1_before_spicesf / total_spicesf\n",
        "top5_before_spicesf = 100. * correct5_before_spicesf / total_spicesf\n",
        "top1_after_spicesf = 100. * correct1_after_spicesf / total_spicesf\n",
        "top5_after_spicesf = 100. * correct5_after_spicesf / total_spicesf\n",
        "\n",
        "improvement1sf = top1_after_spicesf - top1_before_spicesf\n",
        "improvement5sf = top5_after_spicesf - top5_before_spicesf\n",
        "\n",
        "# reset in general\n",
        "olm4.classifier.weight.data = og_classifier_weightf.clone()\n",
        "if og_classifier_biasf is not None:\n",
        "    olm4.classifier.bias.data = og_classifier_biasf.clone()\n",
        "olm4.piriform.w.data = og_piriform_wf.clone()\n",
        "\n"
      ],
      "metadata": {
        "id": "rHG29pyvpbU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print"
      ],
      "metadata": {
        "id": "tZJjlW-vYzE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"online testing - nuts\")\n",
        "print(\"\\nAbsolute Readings Model:\")\n",
        "print(f\"  Top-1 Accuracy Before: {top1_before_nuts:.2f}%\")\n",
        "print(f\"  Top-1 Accuracy After:  {top1_after_nuts:.2f}%\")\n",
        "print(f\"  Top-1 Improvement:     {improvement1na:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy Before: {top5_before_nuts:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy After:  {top5_after_nuts:.2f}%\")\n",
        "print(f\"  Top-5 Improvement:     {improvement5na:.2f}%\")\n",
        "\n",
        "print(\"\\nFOTD Model:\")\n",
        "print(f\"  Top-1 Accuracy Before: {top1_before_nutsf:.2f}%\")\n",
        "print(f\"  Top-1 Accuracy After:  {top1_after_nutsf:.2f}%\")\n",
        "print(f\"  Top-1 Improvement:     {improvement1nf:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy Before: {top5_before_nutsf:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy After:  {top5_after_nutsf:.2f}%\")\n",
        "print(f\"  Top-5 Improvement:     {improvement5nf:.2f}%\")\n",
        "\n",
        "print(\"online testing - spices\")\n",
        "print(\"\\nAbsolute Readings Model:\")\n",
        "print(f\"  Top-1 Accuracy Before: {top1_before_spices:.2f}%\")\n",
        "print(f\"  Top-1 Accuracy After:  {top1_after_spices:.2f}%\")\n",
        "print(f\"  Top-1 Improvement:     {improvement1sa:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy Before: {top5_before_spices:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy After:  {top5_after_spices:.2f}%\")\n",
        "print(f\"  Top-5 Improvement:     {improvement5sa:.2f}%\")\n",
        "\n",
        "print(\"\\nFOTD Model:\")\n",
        "print(f\"  Top-1 Accuracy Before: {top1_before_spicesf:.2f}%\")\n",
        "print(f\"  Top-1 Accuracy After:  {top1_after_spicesf:.2f}%\")\n",
        "print(f\"  Top-1 Improvement:     {improvement1sf:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy Before: {top5_before_spicesf:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy After:  {top5_after_spicesf:.2f}%\")\n",
        "print(f\"  Top-5 Improvement:     {improvement5sf:.2f}%\")\n",
        "\n",
        "\n",
        "# offline testing by cat\n",
        "print(\"offline testing\")\n",
        "\n",
        "# absolute model\n",
        "print(\"\\nAbsolute Readings Model:\")\n",
        "test_cat_results_abs = evaluate_by_category(\n",
        "    olm1, offline_testing_data, offline_testing_labels,\n",
        "    device, ingredient_to_category, label_to_odor, mean =\n",
        "    train_mean, std = train_std, use_fotd = False, max_len = test_dataset.max_len,\n",
        "    k=5\n",
        ")\n",
        "\n",
        "for category in sorted(test_cat_results_abs.keys()):\n",
        "    results = test_cat_results_abs[category]\n",
        "    print(f\"\\n{category}:\")\n",
        "    print(f\" Samples: {results['n_samples']}\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1_acc']:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5_acc']:.2f}%\")\n",
        "\n",
        "# fOTD model\n",
        "print(\"\\nFOTD Model:\")\n",
        "test_cat_results_fotd = evaluate_by_category(\n",
        "    olm2, offline_testing_data, offline_testing_labels,\n",
        "    device, ingredient_to_category, label_to_odor,\n",
        "    mean = train_mean, std = train_std, use_fotd = True,\n",
        "    p = 25, max_len = train_datasetf.max_len, k=5\n",
        ")\n",
        "\n",
        "for category in sorted(test_cat_results_fotd.keys()):\n",
        "    results = test_cat_results_fotd[category]\n",
        "    print(f\"\\n{category}:\")\n",
        "    print(f\" Samples: {results['n_samples']}\")\n",
        "    print(f\" Top-1 Accuracy: {results['top1_acc']:.2f}%\")\n",
        "    print(f\" Top-5 Accuracy: {results['top5_acc']:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "L6cm8_OAYzm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "oGUhcWXZvD7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss plt\n",
        "plt.figure(figsize = (24,16))\n",
        "plt.subplot(3,4,1)\n",
        "plt.plot(results1['train_losses'], label = 'Absolute')\n",
        "plt.plot(results2['train_losses'], label = 'FOTD')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha = 0.3)\n",
        "\n",
        "# accuracy plt\n",
        "plt.subplot(3,4,2)\n",
        "plt.plot(results1['train_accs'], label = 'Absolute')\n",
        "plt.plot(results2['train_accs'], label = 'FOTD')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training acc')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha = 0.3)\n",
        "\n",
        "# test top1 acc\n",
        "plt.subplot(3,4,3)\n",
        "plt.plot(results1['test_top1accs'], label = 'Absolute')\n",
        "plt.plot(results2['test_top1accs'], label = 'FOTD')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('top 1 Accuracy (%)')\n",
        "plt.title('Testing top 1 accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha = 0.3)\n",
        "\n",
        "# test top5 acc\n",
        "plt.subplot(3,4,4)\n",
        "plt.plot(results1['test_top5accs'], label = 'Absolute')\n",
        "plt.plot(results2['test_top5accs'], label = 'FOTD')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('top 5 Accuracy (%)')\n",
        "plt.title('Testing top 5 accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha = 0.3)\n",
        "\n",
        "\n",
        "\n",
        "# online learning for nuts\n",
        "plt.subplot(3, 4, 5)\n",
        "categories = ['Before', 'After']\n",
        "abs_nuts_top1 = [top1_before_nuts, top1_after_nuts]\n",
        "fotd_nuts_top1 = [top1_before_nutsf, top1_after_nutsf]\n",
        "x = np.arange(len(categories))\n",
        "width = 0.35\n",
        "plt.bar(x - width/2, abs_nuts_top1, width, label='Absolute', alpha=0.8)\n",
        "plt.bar(x + width/2, fotd_nuts_top1, width, label='FOTD', alpha=0.8)\n",
        "plt.xlabel('Adaptation Stage', fontsize=12)\n",
        "plt.ylabel('Top1 Accuracy (%)', fontsize=12)\n",
        "plt.title('online learning - nuts', fontsize=14, fontweight='bold')\n",
        "plt.xticks(x, categories)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.subplot(3, 4, 6)\n",
        "abs_nuts_top5 = [top5_before_nuts, top5_after_nuts]\n",
        "fotd_nuts_top5 = [top5_before_nutsf, top5_after_nutsf]\n",
        "plt.bar(x - width/2, abs_nuts_top5, width, label='Absolute', alpha=0.8)\n",
        "plt.bar(x + width/2, fotd_nuts_top5, width, label='FOTD', alpha=0.8)\n",
        "plt.xlabel('Adaptation Stage', fontsize=12)\n",
        "plt.ylabel('Top5 Accuracy (%)', fontsize=12)\n",
        "plt.title('online learning - nuts', fontsize=14, fontweight='bold')\n",
        "plt.xticks(x, categories)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# online learning for spices\n",
        "plt.subplot(3, 4, 7)\n",
        "abs_spices_top1 = [top1_before_spices, top1_after_spices]\n",
        "fotd_spices_top1 = [top1_before_spicesf, top1_after_spicesf]\n",
        "plt.bar(x - width/2, abs_spices_top1, width, label='Absolute', alpha=0.8)\n",
        "plt.bar(x + width/2, fotd_spices_top1, width, label='FOTD', alpha=0.8)\n",
        "plt.xlabel('Adaptation Stage', fontsize=12)\n",
        "plt.ylabel('Top1 Accuracy (%)', fontsize=12)\n",
        "plt.title('online learning - spices', fontsize=14, fontweight='bold')\n",
        "plt.xticks(x, categories)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.subplot(3, 4, 8)\n",
        "abs_spices_top5 = [top5_before_spices, top5_after_spices]\n",
        "fotd_spices_top5 = [top5_before_spicesf, top5_after_spicesf]\n",
        "plt.bar(x - width/2, abs_spices_top5, width, label='Absolute', alpha=0.8)\n",
        "plt.bar(x + width/2, fotd_spices_top5, width, label='FOTD', alpha=0.8)\n",
        "plt.xlabel('Adaptation Stage', fontsize=12)\n",
        "plt.ylabel('Top5 Accuracy (%)', fontsize=12)\n",
        "plt.title('online learning - spices', fontsize=14, fontweight='bold')\n",
        "plt.xticks(x, categories)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "\n",
        "\n",
        "# by category\n",
        "plt.subplot(3, 4, 9)\n",
        "categories = sorted(test_cat_results_abs.keys())\n",
        "abs_top1 = [test_cat_results_abs[cat]['top1_acc'] for cat in categories]\n",
        "fotd_top1 = [test_cat_results_fotd[cat]['top1_acc'] for cat in categories]\n",
        "x = np.arange(len(categories))\n",
        "width = 0.35\n",
        "plt.bar(x - width/2, abs_top1, width, label='Absolute', alpha=0.8)\n",
        "plt.bar(x + width/2, fotd_top1, width, label='FOTD', alpha=0.8)\n",
        "plt.xlabel('Category', fontsize=12)\n",
        "plt.ylabel('top1 Accuracy (%)', fontsize=12)\n",
        "plt.title('category performance', fontsize=14, fontweight='bold')\n",
        "plt.xticks(x, categories, rotation=45, ha='right')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.subplot(3, 4, 10)\n",
        "abs_top5 = [test_cat_results_abs[cat]['top5_acc'] for cat in categories]\n",
        "fotd_top5 = [test_cat_results_fotd[cat]['top5_acc'] for cat in categories]\n",
        "plt.bar(x - width/2, abs_top5, width, label='Absolute', alpha=0.8)\n",
        "plt.bar(x + width/2, fotd_top5, width, label='FOTD', alpha=0.8)\n",
        "plt.xlabel('Category', fontsize=12)\n",
        "plt.ylabel('top5 Accuracy (%)', fontsize=12)\n",
        "plt.title('category performance', fontsize=14, fontweight='bold')\n",
        "plt.xticks(x, categories, rotation=45, ha='right')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.subplot(3, 4, 11)\n",
        "n_samples = [test_cat_results_abs[cat]['n_samples'] for cat in categories]\n",
        "plt.bar(categories, n_samples, alpha=0.8, color='steelblue')\n",
        "plt.xlabel('Category', fontsize=12)\n",
        "plt.ylabel('Number of Samples', fontsize=12)\n",
        "plt.title('Test Set Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('olm_comprehensive_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# saving model\n",
        "save_dir = 'content/drive/MyDrive/3500final'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "torch.save(olm1.state_dict(), os.path.join(save_dir, 'olm_model1.pth'))\n",
        "print(\"\\nModel saved as 'olm_model1.pth\")\n",
        "torch.save(olm2.state_dict(), os.path.join(save_dir, 'olm_model2.pth'))\n",
        "print(\"\\nModel saved as 'olm_model2.pth\")\n",
        "torch.save(olm3.state_dict(), os.path.join(save_dir, 'olm_model3.pth'))\n",
        "print(\"\\nModel saved as 'olm_model3.pth\")\n",
        "torch.save(olm4.state_dict(), os.path.join(save_dir, 'olm_model4.pth'))\n",
        "print(\"\\nModel saved as 'olm_model4.pth\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WasMQDAwvDT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}